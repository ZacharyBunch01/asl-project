{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8aa31-221a-41ea-a3ed-c53c32173919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "# Local project imports\n",
    "from dataloaders import get_dataloaders\n",
    "from model import build_model\n",
    "from transforms import get_video_transform\n",
    "from predict import predict  # if predict.py is in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae31a3f0-a723-4e8c-af63-b7fc3a36ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "root = Path(\"../../../Data/Visual_split/\")\n",
    "\n",
    "print(\"Root exists:\", root.exists())\n",
    "print(\"Subdirectories:\", [p.name for p in root.iterdir() if p.is_dir()])\n",
    "\n",
    "train_dir = root / \"train\"\n",
    "test_dir = root / \"test\"\n",
    "print(\"\\nTrain classes:\", [p.name for p in train_dir.iterdir() if p.is_dir()])\n",
    "print(\"\\nTest classes:\", [p.name for p in test_dir.iterdir() if p.is_dir()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b861fe-bc52-4730-9430-5c282e5f26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    " VideoDataset: How ASL clips are loaded\n",
    "\n",
    "The `VideoDataset` class (defined in `dataset.py` or together with `dataloaders.py`) is responsible for:\n",
    "\n",
    "1. Discovering all classes:\n",
    "   - It scans `root_dir` and collects all subfolders as class names.\n",
    "   - It sorts them and builds `class_to_idx` (e.g., `{\"HELLO\": 0, \"PLEASE\": 1, ...}`).\n",
    "\n",
    "2. Collecting all video samples:\n",
    "   - For each class folder, it finds all `.mp4` files.\n",
    "   - It stores a list of `(video_path, label_index)` pairs in `self.samples`.\n",
    "\n",
    "3. Loading and sampling frames from a video:\n",
    "   - Uses `torchvision.io.read_video` to read the video into a tensor of shape `(T, H, W, C)`.\n",
    "   - Uniformly samples a fixed number of frames (`num_frames`, default 16).\n",
    "   - If the video is too short, it repeats frames to reach 16.\n",
    "   - Converts frames to float in `[0, 1]` and permutes to `(T, C, H, W)`.\n",
    "   - Applies image transforms **frame-by-frame** (resize, normalize, etc.).\n",
    "\n",
    "This turns each raw `.mp4` ASL clip into a tensor of shape `(T, C, H, W)` that can be fed into a 3D CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c3729-626a-4cea-9cac-afdf8edd3b50",
   "metadata": {},
   "outputs": [],
   "source": [
    " – Code: `model.py`\n",
    "\n",
    "```python\n",
    "# model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
    "\n",
    "def build_model(num_classes, pretrained=True):\n",
    "    if pretrained:\n",
    "        weights = R3D_18_Weights.DEFAULT\n",
    "        model = r3d_18(weights=weights)\n",
    "    else:\n",
    "        model = r3d_18(weights=None)\n",
    "\n",
    "    # Replace final FC layer\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d863c0-4225-40b3-b991-f4139c8d5632",
   "metadata": {},
   "outputs": [],
   "source": [
    " `build_model`: 3D ResNet for ASL\n",
    "\n",
    "- Uses `r3d_18`, a **3D ResNet-18** model from `torchvision.models.video`.\n",
    "- If `pretrained=True`, it loads `R3D_18_Weights.DEFAULT`, which are weights pretrained on a large action recognition dataset.\n",
    "\n",
    "Why this helps ASL:\n",
    "\n",
    "- The pretrained model already knows how to detect **motion patterns** and **spatio-temporal features** from videos.\n",
    "- By replacing the final fully-connected layer (`model.fc`) with a new linear layer with `num_classes` outputs, we adapt it to predict **ASL sign classes** instead of general actions.\n",
    "\n",
    "So the model output is:\n",
    "\n",
    "- A vector of length `num_classes` with **logits** (one per ASL class).\n",
    "- During training, we use cross-entropy loss to map those logits to the correct ASL label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29dd9b-0f32-4c2c-8765-9b7859803f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "`predict.py`: Inference on a New ASL Video\n",
    "\n",
    "This script runs **inference** on a single `.mp4` ASL video.\n",
    "\n",
    "Loading the checkpoint\n",
    "\n",
    "```python\n",
    "ckpt = torch.load(checkpoint, map_location=device)\n",
    "class_names = ckpt[\"class_names\"]\n",
    "model = build_model(num_classes=len(class_names), pretrained=False)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ac1f9-3ea3-4529-856f-1cdf49337013",
   "metadata": {},
   "outputs": [],
   "source": [
    " Training & Transforms: How Everything Connects\n",
    "\n",
    "`get_video_transform` (frame-level transforms)\n",
    "\n",
    "- Resizes each frame to `112 x 112`.\n",
    "- Normalizes using standard video statistics (same as Kinetics models).\n",
    "- Optionally, we could enable `RandomHorizontalFlip`, but for ASL, flipping can change meaning, so it’s commented out.\n",
    "\n",
    "This transform is used in **both**:\n",
    "\n",
    "- `VideoDataset` (training and testing)\n",
    "- `load_video_tensor` in `predict.py` (inference)\n",
    "\n",
    "So ASL videos are always preprocessed in a consistent way.\n",
    "\n",
    " `train_one_epoch` and `eval_model`\n",
    "\n",
    "Both functions:\n",
    "\n",
    "- Permute videos from `(B, T, C, H, W)` to `(B, C, T, H, W)`:\n",
    "\n",
    "  ```python\n",
    "  videos = videos.permute(0, 2, 1, 3, 4).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329957b-bae7-4329-bcc4-b71266892afd",
   "metadata": {},
   "outputs": [],
   "source": [
    " Summary & Next Steps\n",
    "\n",
    "\n",
    "- Described the **ASL dataset structure**, where each class corresponds to a specific sign.\n",
    "- Explained how `VideoDataset`:\n",
    "  - Discovers ASL classes from folders\n",
    "  - Loads `.mp4` clips\n",
    "  - Samples a fixed number of frames\n",
    "  - Applies frame-wise transforms for the 3D CNN\n",
    "- Showed how `get_dataloaders` builds training and test dataloaders.\n",
    "- Documented how the **3D ResNet (R3D-18)** model is adapted for ASL classification by replacing the final layer.\n",
    "- Reviewed the training loop and how the model is saved to `video_classifier.pt`.\n",
    "- Ran (or at least documented) the prediction pipeline to classify a new ASL video clip.\n",
    "\n",
    "### Possible extensions\n",
    "\n",
    "- Add more ASL signs and collect more training data.\n",
    "- Experiment with:\n",
    "  - Different numbers of frames (e.g., 8, 32)\n",
    "  - Data augmentation (carefully, due to ASL semantics)\n",
    "  - Different architectures (e.g., `mc3_18`, `r2plus1d_18`)\n",
    "- Generate confusion matrices and per-class metrics to understand which ASL signs are harder for the model.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
